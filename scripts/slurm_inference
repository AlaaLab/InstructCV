#!/bin/bash
#SBATCH -o %j.out
#SBATCH --gres=gpu:1
#SBATCH -p gpu32
#SBATCH --qos=normal
#SBATCH -J test
#SBATCH --nodes=1 
#SBATCH -t 48:00:00

#source activate esmfold
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_seg4.jpeg --edit "segment the person"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_seg2.jpeg --edit "segment the image"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_seg3.jpeg --edit "segment all"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_det13.jpeg --edit "detect the woody"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_det14.jpeg --edit "detect the person"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_det15.jpeg --edit "detect the man"
# python edit_cli.py --input imgs/woody.jpeg --output imgs/woody_detect2.jpeg --edit "detect the face"

## for det:
# python edit_cli.py --input imgs_test_coco/ --output imgs/ --edit "detect the person"

## for seg:
# python edit_cli.py --input data/ade20k/images/ADE/training/cultural/apse__indoor/ADE_train_00001472.jpg --output imgs/ADE_train_00001472_seg.jpeg --edit "segment the person"

## for cls:
## for evaluation
# python edit_cli.py --ckpt logs/train_pets_cls_cls/checkpoints/last.ckpt --resolution 256 --input data/oxford-pets --output outputs/img_test_cls --edit "show black if contains dog, otherwise show white" --split test.txt --task cls
## for debug
CUDA_VISIBLE_DEVICES=6 python edit_cli.py --split "test_part4.txt" --ckpt logs/train_all100kdata_rephrased_newnew/checkpoints/last.ckpt --resolution 256 --input data/ADEChallengeData2016/ --output outputs/img_test_ade20k_rephrasedv2 --edit "Perform image segmentation to extract the % using different colors." --task seg

## for depth estimation
# python edit_cli_depes.py --input /lustre/grp/gyqlab/lism/brt/language-vision-interface/data/nyuv2/basement_0001a --output imgs_test_nyuv2/ --edit "Assess the image's depth by looking for visual cues and making an estimation."


## for pets
# python edit_cli_cls.py --resolution 256 --ckpt logs/train_all100kdata_new/checkpoints/epoch=000051.ckpt --input data/oxford-pets/images --output outputs/imgs_test_oxford_pets/ --edit "show blue if the picture contain %" --task cls
# python edit_cli_seg.py --input data/oxford-pets/images --output imgs_test_oxford_pets/ --edit "segment the %" --task seg
# python edit_cli_det.py --ckpt logs/train_pets_nyuv2_four_tasks_depsprompt:image/checkpoints/last.ckpt --input data/oxford-pets/images --output data/image_pairs_evaluation_det --edit "detect the %" --task det

## for nyuv2
# python edit_cli.py --resolution 256 --ckpt logs/train_all100kdata_add_coco_pet_seg/checkpoints/epoch=000015.ckpt --input data/nyu_mdet/ --output "outputs/imgs_test_nyuv2" --edit "Estimate the depth of this image" --task depes

## for ade20k
# CUDA_VISIBLE_DEVICES=7 python edit_cli.py --resolution 256 --split "test_part4.txt" --ckpt logs/train_test_panotic_seg_voc_ade_2/checkpoints/epoch=000021.ckpt --input data/ADEChallengeData2016/ --output outputs/imgs_test_ade20k_rephrase_0517_2/ --edit "segment the % using different colors." --task seg

## for voc seg
# CUDA_VISIBLE_DEVICES=2 python edit_cli.py --resolution 256 --split "val_part5.txt" --ckpt logs/train_all100kdata_add_coco_pet_seg/checkpoints/epoch=000038.ckpt --input /lustre/grp/gyqlab/lism/brt/language-vision-interface/data/VOCdevkit/VOC2012 --output outputs/imgs_test_voc  --edit "segment the %" --task voc_seg

## for voc det
# python edit_cli.py --resolution 256 --split "val_part0.txt" --ckpt logs/train_all100kdata_add_coco_pet_seg/checkpoints/epoch=000030_.ckpt --input /lustre/grp/gyqlab/lism/brt/language-vision-interface/data/VOCdevkit/VOC2012 --output outputs/imgs_test_voc_det  --edit "detect the %" --task voc_det

# for coco
CUDA_VISIBLE_DEVICES=7 python edit_cli.py --resolution 512 --split "test_part7.txt" --ckpt logs/train_all100kdata_add_coco_pet_seg_blue/checkpoints/last.ckpt --input data/coco --output ./outputs/imgs_test_coco_new2/ --edit "detect the %" --task det

# for unified-io 
# CUDA_VISIBLE_DEVICES=0 python baselines/unified-io-inference/demo_script.py

# for fs-1000
# python edit_cli.py --resolution 256 --ckpt logs/train_all100kdata_add_coco_pet_seg/checkpoints/epoch=000030_.ckpt --input data/fewshot_data/fewshot_data --output outputs/imgs_test_fs1000/ --edit "segment the %" --task fs1000_seg
## unified-io
# python baselines/unified-io-inference/demo_script_seg_fs1000.py

# for sunrgbd
# CUDA_VISIBLE_DEVICES=4 python edit_cli.py --test_txt_path data/SUNRGBD/SUNRGBD_val_splits4.txt --resolution 256 --ckpt logs/train_all100kdata_new/checkpoints/epoch=000051.ckpt --input data/ --output outputs/imgs_test_sunrgbd/ --edit "Estimate the depth of this image" --task sunrgbd_depes
## unified-io

# for pet seg
CUDA_VISIBLE_DEVICES=7 python edit_cli.py --resolution 256 --ckpt logs/train_all100kdata_add_coco_pet_seg_blue/checkpoints/epoch=000020.ckpt --input data/oxford-pets --output ./outputs/imgs_test_pets_seg/ --edit "segment the %" --task pet_seg


# for fig1
python edit_cli.py --ckpt logs/train_all100kdata_add_coco_pet_seg_blue/checkpoints/epoch=000020.ckpt --resolution 512 --input outputs/test_one_image --output outputs/img_test_cls --edit "detect the woody's face" --task det --single_test

## for vis_depes
# python edit_cli.py --ckpt logs/train_nyuv2_depes/checkpoints/epoch=000049.ckpt --input visualization/dep_img --output outputs/visualization_depes --edit "Estimate the depth of this image" --task depes --single_test